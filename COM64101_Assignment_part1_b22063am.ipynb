{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarmohammed398/COM64101_Assignment_part1_b22063am/blob/main/COM64101_Assignment_part1_b22063am.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf52PGSevKXY"
      },
      "source": [
        "# Assignment Brief: Statistical Inference and Gaussian processes\n",
        "\n",
        "## Deadline: November 18, 2025, 14:00 GMT\n",
        "\n",
        "## Number of marks available: 35\n",
        "\n",
        "This coursework is made of two parts. In the first part, you will explore different techniques for approximate inference. In the second part, you will use Bayesian optimisation for hyperparameter learning.\n",
        "\n",
        "### Please READ the whole assignment first, before starting to work on it.\n",
        "\n",
        "### How and what to submit\n",
        "\n",
        "A. A **Jupyter Notebook** with the code in all the cells executed and outputs displayed.\n",
        "\n",
        "B. Name your Notebook **COM64101_Assignment_part1_XXXXXX.ipynb** where XXXXXX is your username such as such as abc18de. Example: `COM64101_Assignment_abc18de.ipynb`\n",
        "\n",
        "C. Upload the Jupyter Notebook in B to Canvas under the submission area before the deadline.\n",
        "\n",
        "D. **NO DATA UPLOAD**: Please do not upload the data files used in this Notebook. We have a copy already.\n",
        "\n",
        "\n",
        "### Assessment Criteria\n",
        "\n",
        "* Being able to correctly apply frequentist, Bayesian, Monte Carlo, importance sampling, and variational inference methods as specified in each task.\n",
        "\n",
        "* Being able to provide clear comparisons between methods (e.g., MLE vs. Bayesian, plain MC vs. IS, VI vs. HMC) using appropriate metrics, plots, and variance/uncertainty evaluations.\n",
        "\n",
        "* Being able to use Gaussian processes as a surrogate model for Bayesian optimisation of the hyperparameters of a machine learning model.\n",
        "\n",
        "* Being able to concisely explain results, justify methodological choices, and discuss observed differences within the given word limits.\n",
        "\n",
        "### Code quality and use of Python libraries\n",
        "When writing your code, you will find out that there are operations that are repeated at least twice. If your code is unreadable, we may not award marks for that section. Make sure to check the following:\n",
        "\n",
        "* Did you include Python functions to solve the question and avoid repeating code?\n",
        "* Did you comment your code to make it readable to others?\n",
        "\n",
        "### Late submissions\n",
        "\n",
        "We follow Department's guidelines about late submissions, i.e., a deduction of 10% of the mark each 24 hours the work is late after the deadline. NO late submission will be marked one week after the deadline. Please read [this link](https://documents.manchester.ac.uk/display.aspx?DocID=29825).\n",
        "\n",
        "### Academic malpractice\n",
        "\n",
        "**Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.** Please carefully read [what constitutes Academic Malpractice](https://documents.manchester.ac.uk/display.aspx?DocID=2870) if not sure. If you still have questions, please ask your Personal tutor or the Lecturers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVkJ8WgJB5Iu",
        "outputId": "ac7d54de-1c19-4f30-ba75-aca4a7ff3b98"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjSTJn07PvUe"
      },
      "source": [
        "# 1. Statistical Inference (20 marks)\n",
        "\n",
        "Section 1 of this coursework is made of three parts (Part 1.1, Part 1.2, and Part 1.3). Complete the tasks below. Do **not** modify the provided datasets.\n",
        "\n",
        "For Part 1.1, you need to analyze coin-flip data using both frequentist (MLE + Wald CI) and Bayesian (Beta prior + posterior sampling) methods, compare their predictions for future flips, and understand the differences. For Part 1.2, you need to estimate a 2D integral using plain Monte Carlo and importance sampling. For Part 1.3, you need to fit Bayesian logistic regression using both mean-field variational inference and HMC, compare their posterior approximations (via KL divergence and test log-loss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWpjKdzy31rb"
      },
      "source": [
        "## 1.1 Bayesian vs. Frequentist Modelling (5 marks)\n",
        "\n",
        "In this exercise, you will explore two different statistical paradigms—**frequentist inference** and **Bayesian inference**—applied to the problem of estimating a coin’s probability of landing heads. The dataset `coin_experiments.csv` contains 1,000 independent coin-flip experiments, each recording the number of **successes** (heads) and the total **trials**.  \n",
        "\n",
        "**Question**\n",
        "\n",
        "**1.1.A.** The frequentist approach treats the probability of success, $p$, as a fixed but unknown quantity. You will:  \n",
        "- Import the data, print its shape, and check basic sanity (e.g., no missing values, successes ≤ trials). **(0.5 pt)**  \n",
        "- Compute the **Maximum Likelihood Estimate (MLE)**, $\\hat{p} = S/N$, where $S$ is the total number of successes and $N$ is the total number of trials. **(1 pt)**\n",
        "-  Estimate the standard error,  \n",
        "  $$\\text{SE} = \\sqrt{\\hat{p}(1-\\hat{p})/N},$$  \n",
        "  and form a **95% Wald confidence interval** using $z = 1.96$. **(0.5 pt)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KZRxqXW31rc"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ml3E69rh31rc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "e95298e5-8fe4-47d9-b08f-7089442fda27"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ab07b5c4-4962-4ee4-95e0-b5ef77776f09\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ab07b5c4-4962-4ee4-95e0-b5ef77776f09\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3861013219.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"coin_experiments.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# upload coin_experiments.csv\n",
        "try:\n",
        "    from google.colab import files\n",
        "    _ = files.upload()\n",
        "    csv_path = \"coin_experiments.csv\"\n",
        "except Exception:\n",
        "    csv_path = \"coin_experiments.csv\"\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# print shape and run basic sanity checks\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(df.head(3), \"\\n\")\n",
        "\n",
        "missing_total = df.isna().sum().sum()\n",
        "assert missing_total == 0, f\"Dataset has {missing_total} missing values.\"\n",
        "\n",
        "# Ensure required columns exist\n",
        "assert {\"successes\", \"trials\"}.issubset(df.columns), \"Expected 'successes' and 'trials' columns.\"\n",
        "\n",
        "# Ensure counts are nonnegative integers and successes ≤ trials\n",
        "assert (df[\"successes\"] >= 0).all(), \"Negative successes found.\"\n",
        "assert (df[\"trials\"] >= 0).all(), \"Negative trials found.\"\n",
        "assert np.allclose(df[\"successes\"], df[\"successes\"].round()), \"Non-integer successes found.\"\n",
        "assert np.allclose(df[\"trials\"], df[\"trials\"].round()), \"Non-integer trials found.\"\n",
        "assert (df[\"successes\"] <= df[\"trials\"]).all(), \"Found rows with successes > trials.\"\n",
        "\n",
        "print(\"Sanity checks passed\\n\")\n",
        "\n",
        "# Compute S, N, p-hat (MLE), SE, and 95% Wald CI\n",
        "S = int(df[\"successes\"].sum())\n",
        "N = int(df[\"trials\"].sum())\n",
        "p_hat = S / N\n",
        "\n",
        "SE = np.sqrt(p_hat * (1 - p_hat) / N)\n",
        "z = 1.96  # 95% CI\n",
        "ci_low = p_hat - z * SE\n",
        "ci_high = p_hat + z * SE\n",
        "\n",
        "# print results\n",
        "print(\"=== Frequentist Estimate for Coin's Head Probability ===\")\n",
        "print(f\"Total successes (S): {S}\")\n",
        "print(f\"Total trials    (N): {N}\")\n",
        "print(f\"MLE p-hat (S/N)     : {p_hat:.6f}\")\n",
        "print(f\"Standard Error      : {SE:.6f}\")\n",
        "print(f\"95% Wald CI         : [{ci_low:.6f}, {ci_high:.6f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P87qT6X_31rc"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.1.B.** The Bayesian approach treats $p$ as a **random variable** with prior distribution. Using a **Beta(1, 1)** prior (uniform), you will:  \n",
        "- Compute the posterior parameters,  \n",
        "  $$\\alpha_{\\text{post}} = \\alpha_0 + S, \\quad \\beta_{\\text{post}} = \\beta_0 + (N-S),$$  \n",
        "  and the **MAP estimate**,  \n",
        "  $$p_{\\text{MAP}} = \\frac{\\alpha_{\\text{post}} - 1}{\\alpha_{\\text{post}} + \\beta_{\\text{post}} - 2}.$$\n",
        "  **(1 pt)**\n",
        "- Draw at least 5,000 samples from the Beta posterior (e.g., `scipy.stats.beta.rvs`). **(0.5 pt)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpEo_w8C31rc"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0onaReD31rd"
      },
      "outputs": [],
      "source": [
        "# --- Bayesian Beta-Binomial with Beta(1,1) prior (no repeated imports) ---\n",
        "\n",
        "# Posterior parameters (conjugate update)\n",
        "alpha0, beta0 = 1, 1\n",
        "alpha_post = alpha0 + S\n",
        "beta_post  = beta0 + (N - S)\n",
        "\n",
        "# MAP estimate (valid since alpha_post > 1 and beta_post > 1 here)\n",
        "p_map = (alpha_post - 1) / (alpha_post + beta_post - 2)\n",
        "\n",
        "# Draw >= 5,000 samples from Beta(alpha_post, beta_post)\n",
        "rng = np.random.default_rng(42)     # reproducible\n",
        "num_samples = 10_000                # ≥ 5,000 as requested\n",
        "samples = rng.beta(alpha_post, beta_post, size=num_samples)\n",
        "\n",
        "# Posterior summaries (from both conjugacy and samples)\n",
        "post_mean_closed = alpha_post / (alpha_post + beta_post)  # exact mean of Beta\n",
        "post_mean_samp   = samples.mean()\n",
        "post_sd_samp     = samples.std(ddof=1)\n",
        "ci2p5, ci97p5    = np.quantile(samples, [0.025, 0.975])\n",
        "\n",
        "# Print results\n",
        "print(\"=== Bayesian Beta-Binomial Update (Beta(1,1) prior) ===\")\n",
        "print(f\"S (total successes): {S}\")\n",
        "print(f\"N (total trials)   : {N}\")\n",
        "print(f\"alpha_post         : {alpha_post}\")\n",
        "print(f\"beta_post          : {beta_post}\")\n",
        "print(f\"MAP estimate       : {p_map:.8f}\")\n",
        "print(f\"Posterior mean (closed-form): {post_mean_closed:.8f}\")\n",
        "print(f\"Posterior mean (samples)    : {post_mean_samp:.8f}\")\n",
        "print(f\"Posterior SD (samples)      : {post_sd_samp:.8f}\")\n",
        "print(f\"95% credible int (samples)  : [{ci2p5:.8f}, {ci97p5:.8f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fCEXSp931rd"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.1.C.** Finally, compare predictions for the next **20 flips**:  \n",
        "- From the Bayesian model, simulate posterior predictive outcomes by sampling from a **Binomial(20, p)** where $p$ comes from the posterior samples, and show a histogram/density. **(0.5 pt)**\n",
        "- From the frequentist model, plot the **plug-in Binomial pmf** with $p = \\hat{p}_{\\text{MLE}}$ over the same figure.**(0.5 pt)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVpH-8j-31rd"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izUeIdkt31rd"
      },
      "outputs": [],
      "source": [
        "# --- 1.1.C  Posterior predictive vs Frequentist plug-in prediction for 20 flips ---\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import binom\n",
        "\n",
        "# ---------- Bayesian posterior predictive ----------\n",
        "# We already have posterior samples: `samples` (size = num_samples)\n",
        "num_future = 20\n",
        "\n",
        "# For each posterior draw p^(s), generate one Binomial(20, p^(s))\n",
        "ppc = rng.binomial(n=num_future, p=samples)\n",
        "\n",
        "# ---------- Frequentist plug-in predictive ----------\n",
        "p_mle = p_hat\n",
        "x_vals = np.arange(0, num_future + 1)\n",
        "freq_pmf = binom.pmf(x_vals, n=num_future, p=p_mle)\n",
        "\n",
        "# ---------- Plot ----------\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# Histogram of posterior predictive counts\n",
        "plt.hist(ppc, bins=np.arange(-0.5, num_future+1.5, 1),\n",
        "         density=True, alpha=0.6, label=\"Bayesian posterior predictive\")\n",
        "\n",
        "# Overlay Frequentist plug-in Binomial pmf\n",
        "plt.plot(x_vals, freq_pmf, 'o-', color='red',\n",
        "         label=f\"Frequentist Binomial(n=20, p={p_mle:.3f})\")\n",
        "\n",
        "plt.xlabel(\"Number of heads in next 20 flips\")\n",
        "plt.ylabel(\"Probability / Density\")\n",
        "plt.title(\"Posterior Predictive vs. Frequentist Plug-in Prediction\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IJEvuQN31rd"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.1.D.** Write a short explanation comparing the two predictive distributions. Comment on how the Bayesian posterior predictive accounts for uncertainty in $p$, while the frequentist plug-in relies on a single point estimate. **(0.5 pt)**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue0njUrP31rd"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "The Bayesian and frequentist preductive distributions look very similar because the dataset is large, giving a precise estimate of the coin's bias. However, they differ conceptually because in the Bayesian posterior predictive method, the model samples values of $p$ from the posterior distribution and then generates predictions for the next 20 flips. This means the prediction integrates over unvertainty in $p$. Even though the posterior is tight, it still has variance, so the Bayesian predictive is slightly wider and reflects remaining uncertainty about the true probability of heads.\n",
        "\n",
        "The frequentist plug-in predictive approach on th other hand uses only the point estimate p^ from the MLE and assumes it is the true value. The resulting Binomial (20, ̂$p$) distribution thereforew ignores parameter uncertainty, giving a slightly narrower predictive distribution.\n",
        "\n",
        "The bayesian posterior predictive accounts for uncertainty in $p$ by averaging over many plausable values, whereas the frequentist plug-in model relies entirely on a single estimate ̂$p$. With large data the two predictions become very close, but the Bayesian approach remains slightly more conservative by incorporating parameter uncertainty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPgvRiCl31rd"
      },
      "source": [
        "## 1.2 Variance Reduction with Importance Sampling (5 marks)\n",
        "\n",
        "Many integrals that arise in statistics and machine learning cannot be evaluated analytically, especially in higher dimensions. **Monte Carlo (MC) integration** is a general technique to approximate such integrals by drawing random samples from a distribution and averaging function evaluations. Although MC estimators are unbiased, their variance can be large, meaning we may need many samples to reach a desired accuracy.  \n",
        "\n",
        "A way to improve efficiency is through **importance sampling (IS)**. Instead of sampling from a fixed distribution (e.g., Gaussian), we choose a proposal distribution $q(x)$ that better matches the regions where the integrand contributes most. By reweighting samples, we can reduce variance while keeping the estimator unbiased. The effectiveness of IS depends critically on choosing $q(x)$ with heavier tails or shapes aligned with the integrand.  \n",
        "\n",
        "In this exercise, you will evaluate a 2D integral and demonstrate how importance sampling can achieve significant variance reduction compared to plain Monte Carlo:  \n",
        "\n",
        "$$\n",
        "I = \\int_{\\mathbb{R}^2} \\exp(-\\|x\\|_1)\\,\\sin(\\|x\\|_2)\\,dx,\n",
        "$$  \n",
        "\n",
        "with the goal of achieving an **absolute error < 0.01**.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRQlnroy31rd"
      },
      "outputs": [],
      "source": [
        "# Visualize the integrand\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def integrand(x):\n",
        "    l1 = np.sum(np.abs(x), axis=-1)   # L1 norm\n",
        "    l2 = np.linalg.norm(x, axis=-1)   # L2 norm\n",
        "    return np.exp(-l1) * np.sin(l2)\n",
        "\n",
        "# grid\n",
        "xx, yy = np.meshgrid(np.linspace(-4,4,400), np.linspace(-4,4,400))\n",
        "pts = np.stack([xx,yy], axis=-1)\n",
        "zz = integrand(pts)\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.pcolormesh(xx, yy, zz, cmap=\"RdBu_r\", shading=\"auto\")\n",
        "plt.colorbar(label=\"f(x)\")\n",
        "plt.title(\"Integrand $f(x) = e^{||x||_1} \\\\, \\\\sin(||x||_2)$\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\")\n",
        "plt.axis(\"equal\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOfetJZM31re"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.2.A. Plain Monte Carlo (2 pts)**  \n",
        "   - Estimate $I$ by sampling $x \\sim \\mathcal{N}(0, I_2)$ using the plain Monte Carlo estimator **(1 pt)**:  \n",
        "     $$\n",
        "     \\hat{I} = \\frac{1}{N} \\sum_{i=1}^N f(x_i), \\quad x_i \\sim \\mathcal{N}(0, I_2).\n",
        "     $$  \n",
        "   - Report your estimate $\\hat{I}$ and the **empirical standard error**. Verify that the error decreases as $N$ increases. **(1 pt)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPT08CY531re"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifb-E5Uf31re"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Integrand (already defined earlier, but redefining here for safety)\n",
        "def integrand(x):\n",
        "    l1 = np.sum(np.abs(x), axis=-1)\n",
        "    l2 = np.linalg.norm(x, axis=-1)\n",
        "    return np.exp(-l1) * np.sin(l2)\n",
        "\n",
        "# Function to run plain Monte Carlo\n",
        "def plain_mc(N, rng=np.random.default_rng()):\n",
        "    # Draw samples x ~ N(0, I2)\n",
        "    x = rng.normal(0, 1, size=(N, 2))\n",
        "    fx = integrand(x)\n",
        "    I_hat = fx.mean()\n",
        "    SE = fx.std(ddof=1) / np.sqrt(N)\n",
        "    return I_hat, SE\n",
        "\n",
        "# Try multiple N to show error decrease\n",
        "Ns = [1_000, 5_000, 20_000, 100_000]\n",
        "results = []\n",
        "\n",
        "print(\"Plain Monte Carlo Estimates:\")\n",
        "for N in Ns:\n",
        "    I_hat, SE = plain_mc(N)\n",
        "    results.append((N, I_hat, SE))\n",
        "    print(f\"N={N:>7}   I_hat={I_hat: .6f}   StdErr={SE: .6f}\")\n",
        "\n",
        "# Optionally store results in a dataframe for nicer display:\n",
        "try:\n",
        "    import pandas as pd\n",
        "    df_results = pd.DataFrame(results, columns=[\"N\", \"I_hat\", \"StdErr\"])\n",
        "    df_results\n",
        "except ImportError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb43K0zU31re"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.2.B Importance Sampling (1.5 pts)**  \n",
        "   - Design your own proposal distribution $q(x)$ and implement an importance sampling estimator.**(1 pt)**\n",
        "   - Demonstrate a **variance reduction** for an equal number of samples **(0.5 pt)**\n",
        "   - *Hint:* Consider proposals (e.g., Laplace, Student-t or other) to better capture the integrand’s structure.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxSx57ik31re"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP6hjyWs31re"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import laplace, t\n",
        "\n",
        "# Integrand (ensure it's available)\n",
        "def integrand(x):\n",
        "    l1 = np.sum(np.abs(x), axis=-1)\n",
        "    l2 = np.linalg.norm(x, axis=-1)\n",
        "    return np.exp(-l1) * np.sin(l2)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CHOICE OF PROPOSAL q(x):\n",
        "# We use an independent 2D Laplace distribution:\n",
        "#   q(x1, x2) = Laplace(0, b) ⊗ Laplace(0, b)\n",
        "# because the integrand is shaped like exp(-||x||_1), making\n",
        "# Laplace a well-matched heavier-tailed proposal.\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "b = 1.0  # Laplace scale parameter\n",
        "\n",
        "def sample_q(N, rng=np.random.default_rng()):\n",
        "    \"\"\" Draw N samples from 2D Laplace proposal q(x). \"\"\"\n",
        "    x1 = rng.laplace(loc=0, scale=b, size=N)\n",
        "    x2 = rng.laplace(loc=0, scale=b, size=N)\n",
        "    return np.column_stack([x1, x2])\n",
        "\n",
        "def log_q(x):\n",
        "    \"\"\" Log-PDF of the 2D independent Laplace(0,b). \"\"\"\n",
        "    # log q(x1,x2) = -log(2b) - |x1|/b  +  same for x2\n",
        "    return -2*np.log(2*b) - (np.abs(x[:,0]) + np.abs(x[:,1]))/b\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Importance Sampling estimator\n",
        "# ---------------------------------------------------------\n",
        "def importance_sampling(N, rng=np.random.default_rng()):\n",
        "    # sample from proposal\n",
        "    x = sample_q(N, rng)\n",
        "\n",
        "    # compute f(x)\n",
        "    fx = integrand(x)\n",
        "\n",
        "    # target density is uniform in integral (i.e., integrating f(x) dx)\n",
        "    # but we need pdf of N(0,I) inside weights? NO:\n",
        "    #   We want ∫ f(x) dx, so weights = f(x) / q(x).\n",
        "    # Only q(x) appears in denominator.\n",
        "    w = fx / np.exp(log_q(x))  # weights = f / q\n",
        "\n",
        "    I_hat = w.mean()\n",
        "    SE = w.std(ddof=1) / np.sqrt(N)\n",
        "    return I_hat, SE\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Compare plain MC vs IS for same N\n",
        "# ---------------------------------------------------------\n",
        "def plain_mc(N, rng=np.random.default_rng()):\n",
        "    x = rng.normal(0, 1, size=(N, 2))\n",
        "    fx = integrand(x)\n",
        "    I_hat = fx.mean()\n",
        "    SE = fx.std(ddof=1) / np.sqrt(N)\n",
        "    return I_hat, SE\n",
        "\n",
        "\n",
        "Ns = [5_000, 20_000, 50_000]\n",
        "results = []\n",
        "\n",
        "print(\"Comparison: Plain MC vs Importance Sampling\\n\")\n",
        "for N in Ns:\n",
        "    I_mc, SE_mc = plain_mc(N)\n",
        "    I_is, SE_is = importance_sampling(N)\n",
        "\n",
        "    results.append((N, I_mc, SE_mc, I_is, SE_is))\n",
        "\n",
        "    print(f\"N = {N}\")\n",
        "    print(f\"  Plain MC:          I_hat = {I_mc: .6f},  StdErr = {SE_mc: .6f}\")\n",
        "    print(f\"  Importance Samp.:  I_hat = {I_is: .6f},  StdErr = {SE_is: .6f}\")\n",
        "    print(f\"  Variance Reduction = {(SE_mc/SE_is)**2: .2f}x\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "# Optional: pretty display in a DataFrame\n",
        "try:\n",
        "    import pandas as pd\n",
        "    df_results = pd.DataFrame(results,\n",
        "                              columns=[\"N\", \"I_MC\", \"SE_MC\", \"I_IS\", \"SE_IS\"])\n",
        "    df_results\n",
        "except ImportError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkzurWa331re"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.2.C RMSE Comparison (1 pt)**  \n",
        "   - Produce a **log–log plot** of RMSE vs. sample size $N$, comparing plain Monte Carlo and importance sampling. **(1 pt)**  \n",
        "   - Use $N \\in [10^3, 10^5]$ (e.g., 5–10 log-spaced points). Ensure both curves are clearly labeled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7J0-uGx31re"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B31l08as31re"
      },
      "outputs": [],
      "source": [
        "# --- 1.2.C RMSE Comparison: Plain MC vs Importance Sampling ---\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reuse integrand and sampling functions from 1.2.A and 1.2.B:\n",
        "\n",
        "def integrand(x):\n",
        "    l1 = np.sum(np.abs(x), axis=-1)\n",
        "    l2 = np.linalg.norm(x, axis=-1)\n",
        "    return np.exp(-l1) * np.sin(l2)\n",
        "\n",
        "# Plain Monte Carlo --------------------------------------------------\n",
        "def plain_mc(N, rng=np.random.default_rng()):\n",
        "    x = rng.normal(0, 1, size=(N, 2))\n",
        "    fx = integrand(x)\n",
        "    I_hat = fx.mean()\n",
        "    SE = fx.std(ddof=1) / np.sqrt(N)\n",
        "    return I_hat, SE\n",
        "\n",
        "# Laplace proposal for IS --------------------------------------------\n",
        "b = 1.0\n",
        "\n",
        "def sample_q(N, rng=np.random.default_rng()):\n",
        "    x1 = rng.laplace(loc=0, scale=b, size=N)\n",
        "    x2 = rng.laplace(loc=0, scale=b, size=N)\n",
        "    return np.column_stack([x1, x2])\n",
        "\n",
        "def log_q(x):\n",
        "    return -2*np.log(2*b) - (np.abs(x[:,0]) + np.abs(x[:,1]))/b\n",
        "\n",
        "def importance_sampling(N, rng=np.random.default_rng()):\n",
        "    x = sample_q(N, rng)\n",
        "    fx = integrand(x)\n",
        "    w = fx / np.exp(log_q(x))   # f(x) / q(x)\n",
        "    I_hat = w.mean()\n",
        "    SE = w.std(ddof=1) / np.sqrt(N)\n",
        "    return I_hat, SE\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Estimate a \"ground truth\" using a very large importance sampler\n",
        "# --------------------------------------------------------------\n",
        "print(\"Computing high-precision reference estimate...\")\n",
        "I_ref, _ = importance_sampling(2_000_000)\n",
        "print(f\"Reference integral ≈ {I_ref:.6f}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# RMSE vs N on a log–log plot\n",
        "# --------------------------------------------------------------\n",
        "Ns = np.logspace(3, 5, 8, dtype=int)  # ~10^3 to 10^5\n",
        "rmse_mc = []\n",
        "rmse_is = []\n",
        "\n",
        "for N in Ns:\n",
        "    I_hat_mc, SE_mc = plain_mc(N)\n",
        "    I_hat_is, SE_is = importance_sampling(N)\n",
        "\n",
        "    rmse_mc.append(np.sqrt((I_hat_mc - I_ref)**2 + SE_mc**2))\n",
        "    rmse_is.append(np.sqrt((I_hat_is - I_ref)**2 + SE_is**2))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.loglog(Ns, rmse_mc, 'o-', label=\"Plain Monte Carlo\")\n",
        "plt.loglog(Ns, rmse_is, 's-', label=\"Importance Sampling (Laplace proposal)\")\n",
        "plt.xlabel(\"Sample size N (log scale)\")\n",
        "plt.ylabel(\"RMSE (log scale)\")\n",
        "plt.title(\"RMSE vs N: Plain MC vs Importance Sampling\")\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnTa8gqW31re"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.2.D. Justification (0.5 pt)**  \n",
        "   - Explain your choice of proposal $q(x)$. Discuss how it aligns with the shape of the integrand and why it reduces variance relative to Gaussian sampling. **(0.5 pt)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0frZTam31rf"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "The proposal distribution ( q(x) ) was chosen to be a 2-dimensional Laplace distribution:\n",
        "\n",
        "$$\n",
        "q(x_1, x_2) = \\text{Laplace}(0,b) \\otimes \\text{Laplace}(0,b).\n",
        "$$\n",
        "\n",
        "This choice aligns naturally with the structure of the integrand:\n",
        "\n",
        "$$\n",
        "f(x) = e^{-\\lVert x \\rVert_1}, \\sin(\\lVert x \\rVert_2),\n",
        "$$\n",
        "\n",
        "whose dominant envelope is\n",
        "\n",
        "$$\n",
        "e^{-\\lVert x \\rVert_1}.\n",
        "$$\n",
        "\n",
        "The Laplace distribution has exactly this exponential ( L_1 )-norm decay, so it places samples in the same regions where the integrand has significant mass. This leads to importance weights\n",
        "\n",
        "$$\n",
        "w(x) = \\frac{f(x)}{q(x)}\n",
        "$$\n",
        "\n",
        "that fluctuate less than when sampling from a Gaussian.\n",
        "\n",
        "By contrast, drawing samples from a Gaussian ( \\mathcal{N}(0,I_2) ) concentrates too heavily near the origin and decays too quickly in the tails. Since the integrand has heavier ( L_1 )-type tails, many Gaussian samples land in regions where ( f(x) \\approx 0 ), producing large variance.\n",
        "\n",
        "Using the Laplace proposal results in samples that better match the shape and tail behavior of ( f(x) ), yielding much smaller variance and lower RMSE for the same number of samples, as observed in the RMSE comparison plot.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSIthCKz31rf"
      },
      "source": [
        "## 1.3 Mean-Field Variational Inference for Bayesian Logistic Regression (10 marks, ★ difficult)\n",
        "\n",
        "In this exercise, you will implement **Bayesian logistic regression** and approximate its posterior distribution using **mean-field variational inference (VI)**. You will then compare VI with a **Hamiltonian Monte Carlo (HMC)** benchmark to highlight the trade-offs between computational efficiency and posterior accuracy. The dataset `log_reg_data.csv` contains predictors `x1, x2, …` and a binary label `y`.\n",
        "\n",
        "### Background & Motivation\n",
        "\n",
        "- **Bayesian logistic regression** models uncertainty in the regression weights $w$, allowing you to quantify predictive uncertainty rather than relying only on a point estimate (as in standard logistic regression). The prior is chosen as a Gaussian:  \n",
        "  $$\n",
        "  w \\sim \\mathcal{N}(0, 10I).\n",
        "  $$  \n",
        "\n",
        "- **Variational Inference (VI)** approximates the true posterior $p(w \\mid D)$ by a simpler distribution. Here, we use a **mean-field Gaussian**:  \n",
        "  $$\n",
        "  q_\\phi(w) = \\mathcal{N}(w \\mid \\mu, \\mathrm{diag}(\\sigma^2)),\n",
        "  $$  \n",
        "  where the parameters $\\phi = (\\mu, \\sigma)$ are optimised to make $q_\\phi$ close to the true posterior.  \n",
        "\n",
        "- Optimisation is done via the **Evidence Lower Bound (ELBO)**:  \n",
        "  $$\n",
        "  \\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi}[ \\log p(D \\mid w) ] - \\mathrm{KL}(q_\\phi \\,\\|\\, p(w)),\n",
        "  $$  \n",
        "  which we estimate stochastically using the **reparameterisation trick**:  \n",
        "  $$\n",
        "  w = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I).\n",
        "  $$  \n",
        "\n",
        "- **Hamiltonian Monte Carlo (HMC)** provides a high-fidelity posterior approximation by simulating from the exact Bayesian posterior using gradient information. It is computationally more expensive but is often treated as the “gold standard” for comparison.  \n",
        "\n",
        "By completing this task, you will see how VI provides a fast but approximate solution, while HMC is slower but more accurate. The comparison highlights the **bias–variance trade-off** in approximate inference.\n",
        "**Hint:**  \n",
        "- Use `autograd` for gradients in VI.  \n",
        "- Clip `log_sigma` to avoid extreme variances.  \n",
        "- Use `scipy.special.expit` for the logistic function.  \n",
        "- For HMC, you can rely on **NumPyro** (already available) to avoid implementing HMC from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE_IJGWD31rf"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.3.A Variational Inference (3 pts)**  \n",
        "   - Construct the VI family and implement the **reparameterised ELBO**. Optimise with SGD/ADAM. **(2 pt)**\n",
        "   - Show a training curve of the ELBO to confirm convergence. **(1 pt)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45p-tW-H31rf"
      },
      "outputs": [],
      "source": [
        "!pip install autograd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggu8qBJz31rf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from autograd import grad\n",
        "import autograd.numpy as anp\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import expit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwhaRoyJ31rf"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0xYdrar31rf"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "# ================================\n",
        "# 1. Load Data\n",
        "# ================================\n",
        "try:\n",
        "    from google.colab import files\n",
        "    _ = files.upload()\n",
        "    csv_path = \"log_reg_data.csv\"\n",
        "except Exception:\n",
        "    csv_path = \"log_reg_data.csv\"\n",
        "\n",
        "data = pd.read_csv(csv_path)\n",
        "X = data.drop(columns='y').values\n",
        "y = data['y'].values\n",
        "\n",
        "# Add intercept\n",
        "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "D = X_train.shape[1]  # number of features including intercept\n",
        "\n",
        "# ================================\n",
        "# 2. Prior and Variational Parameters\n",
        "# ================================\n",
        "prior_mean = anp.zeros(D)\n",
        "prior_var = 10.0\n",
        "\n",
        "np.random.seed(42)\n",
        "mu = np.zeros(D)\n",
        "log_sigma = np.zeros(D)  # optimize log_sigma for stability\n",
        "\n",
        "# ================================\n",
        "# 3. Sigmoid for autograd\n",
        "# ================================\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + anp.exp(-x))\n",
        "\n",
        "# ================================\n",
        "# 4. ELBO function\n",
        "# ================================\n",
        "def elbo(params, X, y, n_samples=10):\n",
        "    mu, log_sigma = params[:D], params[D:]\n",
        "    sigma = anp.exp(log_sigma)\n",
        "    elbo_estimate = 0.\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # Reparameterization trick\n",
        "        epsilon = anp.random.normal(size=D)\n",
        "        w = mu + sigma * epsilon\n",
        "\n",
        "        # Logistic likelihood log p(y|X,w)\n",
        "        logits = anp.dot(X, w)\n",
        "        log_lik = anp.sum(y * anp.log(sigmoid(logits)) + (1 - y) * anp.log(1 - sigmoid(logits)))\n",
        "\n",
        "        # Log prior p(w) ~ N(0, 10I)\n",
        "        log_prior = -0.5 * anp.sum((w**2) / prior_var) - 0.5 * D * anp.log(2 * anp.pi * prior_var)\n",
        "\n",
        "        # Log variational q(w) ~ N(mu, diag(sigma^2))\n",
        "        log_q = -0.5 * anp.sum(((w - mu) / sigma)**2 + 2 * log_sigma + anp.log(2 * anp.pi))\n",
        "\n",
        "        elbo_estimate += log_lik + log_prior - log_q\n",
        "\n",
        "    return -(elbo_estimate / n_samples)  # negative ELBO for minimization\n",
        "\n",
        "# Gradient function\n",
        "elbo_grad = grad(elbo)\n",
        "\n",
        "# ================================\n",
        "# 5. Adam Optimizer\n",
        "# ================================\n",
        "def adam(params, grad_func, X, y, lr=0.01, n_iter=2000, n_samples=10):\n",
        "    m = np.zeros_like(params)\n",
        "    v = np.zeros_like(params)\n",
        "    beta1, beta2 = 0.9, 0.999\n",
        "    eps = 1e-8\n",
        "    losses = []\n",
        "\n",
        "    for i in range(1, n_iter + 1):\n",
        "        grads = grad_func(params, X, y, n_samples)\n",
        "\n",
        "        # Adam update\n",
        "        m = beta1 * m + (1 - beta1) * grads\n",
        "        v = beta2 * v + (1 - beta2) * (grads**2)\n",
        "        m_hat = m / (1 - beta1**i)\n",
        "        v_hat = v / (1 - beta2**i)\n",
        "\n",
        "        params = params - lr * m_hat / (anp.sqrt(v_hat) + eps)\n",
        "\n",
        "        # Clip log_sigma to avoid extreme variances\n",
        "        params[D:] = anp.clip(params[D:], -5, 2)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            current_elbo = -elbo(params, X, y, n_samples)\n",
        "            losses.append(current_elbo)\n",
        "            print(f\"Iter {i}: ELBO = {current_elbo:.3f}\")\n",
        "\n",
        "    return params, losses\n",
        "\n",
        "# ================================\n",
        "# 6. Training VI\n",
        "# ================================\n",
        "params_init = np.concatenate([mu, log_sigma])\n",
        "params_opt, losses = adam(params_init, elbo_grad, X_train, y_train, lr=0.01, n_iter=2000, n_samples=10)\n",
        "\n",
        "# ================================\n",
        "# 7. Plot ELBO training curve\n",
        "# ================================\n",
        "plt.plot(np.arange(50, 2001, 50), losses)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('ELBO')\n",
        "plt.title('ELBO Training Curve')\n",
        "plt.show()\n",
        "\n",
        "# ================================\n",
        "# 8. Extract optimized variational parameters\n",
        "# ================================\n",
        "mu_opt, log_sigma_opt = params_opt[:D], params_opt[D:]\n",
        "sigma_opt = np.exp(log_sigma_opt)\n",
        "\n",
        "print(\"Optimized mu:\", mu_opt)\n",
        "print(\"Optimized sigma:\", sigma_opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRl3RtKX31rf"
      },
      "source": [
        "JAX is a Python library that works like NumPy but adds extra features such as automatic differentiation and faster computations. NumPyro is a library built on JAX that makes it easier to do Bayesian statistics and probabilistic modeling. You should use JAX together with NumPyro, complete the following task:\n",
        "\n",
        "**Question**\n",
        "\n",
        "**1.3.B HMC Benchmark (2 pt)**  \n",
        "   - Run **4 parallel HMC chains** (NumPyro/Stan acceptable) to obtain ≥1,000 effective samples.  \n",
        "   - Report convergence diagnostics ($\\hat{R} \\leq 1.05$).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FORSNVPA31rf"
      },
      "outputs": [],
      "source": [
        "!pip install jax\n",
        "!pip install jaxlib\n",
        "!pip install numpyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zrDZdqB31rg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "from numpyro.diagnostics import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1bh_pBP31rg"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mNF3YIQ31rg"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 2. Load Data\n",
        "# ================================\n",
        "try:\n",
        "    from google.colab import files\n",
        "    _ = files.upload()\n",
        "    csv_path = \"log_reg_data.csv\"\n",
        "except Exception:\n",
        "    csv_path = \"log_reg_data.csv\"\n",
        "\n",
        "data = pd.read_csv(csv_path)\n",
        "X = data.drop(columns='y').values\n",
        "y = data['y'].values\n",
        "\n",
        "# Add intercept\n",
        "X = jnp.hstack([jnp.ones((X.shape[0], 1)), X])\n",
        "D = X.shape[1]  # number of features including intercept\n",
        "\n",
        "# ================================\n",
        "# 3. Define Bayesian Logistic Regression Model\n",
        "# ================================\n",
        "def logistic_regression_model(X, y=None):\n",
        "    # Prior: w ~ N(0, 10*I)\n",
        "    w = numpyro.sample(\"w\", dist.Normal(jnp.zeros(D), jnp.sqrt(10.0) * jnp.ones(D)))\n",
        "    logits = jnp.dot(X, w)\n",
        "    # Likelihood\n",
        "    numpyro.sample(\"obs\", dist.Bernoulli(logits=logits), obs=y)\n",
        "\n",
        "# ================================\n",
        "# 4. Run HMC / NUTS\n",
        "# ================================\n",
        "rng_key = random.PRNGKey(0)\n",
        "nuts_kernel = NUTS(logistic_regression_model)\n",
        "mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=1500, num_chains=4)\n",
        "mcmc.run(rng_key, X=X, y=y)\n",
        "mcmc.print_summary()\n",
        "\n",
        "# ================================\n",
        "# 5. Extract Posterior Samples\n",
        "# ================================\n",
        "posterior_samples = mcmc.get_samples()\n",
        "w_samples = posterior_samples[\"w\"]\n",
        "print(\"Posterior w shape:\", w_samples.shape)  # (chains * samples, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4AvNwpu31rg"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.3.C Posterior Comparison (3 pts)**  \n",
        "   - Compute $\\mathrm{KL}(q_\\phi \\parallel p(w \\mid D))$ using HMC samples as the reference posterior. **(2 pt)**  \n",
        "   - Compute test-set log-loss under both VI and HMC. Present results in a table or plot. **(1 pt)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eICNPVx31rg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "from sklearn.metrics import log_loss\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxcVgM8N31rj"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oWKC-tB31rk"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 1. Variational Posterior Parameters (from your VI training)\n",
        "# ================================\n",
        "mu_vi = params_opt[:D]            # VI mean\n",
        "sigma_vi = np.exp(params_opt[D:]) # VI std deviation\n",
        "\n",
        "# ================================\n",
        "# 2. HMC Posterior Samples (from previous HMC run)\n",
        "# ================================\n",
        "# w_samples: (num_samples_total, D)\n",
        "# Flatten chains if needed\n",
        "w_hmc = np.array(w_samples)  # shape: (num_samples_total, D)\n",
        "num_hmc_samples = w_hmc.shape[0]\n",
        "\n",
        "# ================================\n",
        "# 3. Approximate KL(q || p) using HMC samples\n",
        "# KL(q || p) ≈ 1/S * sum_s log q(w_s) - log p(w_s | D)\n",
        "# ================================\n",
        "def log_q_vi(w, mu, sigma):\n",
        "    return -0.5 * np.sum(((w - mu) / sigma)**2 + 2 * np.log(sigma) + np.log(2 * np.pi), axis=-1)\n",
        "\n",
        "def log_posterior_hmc(w, X, y):\n",
        "    # Log-likelihood\n",
        "    logits = np.dot(X, w.T)  # shape: (N, num_samples)\n",
        "    log_lik = y[:, None] * np.log(expit(logits)) + (1 - y)[:, None] * np.log(1 - expit(logits))\n",
        "    log_lik = np.sum(log_lik, axis=0)\n",
        "    # Log-prior\n",
        "    log_prior = -0.5 * np.sum((w**2)/10.0, axis=1) - 0.5*D*np.log(2*np.pi*10)\n",
        "    return log_lik + log_prior\n",
        "\n",
        "log_q_vals = log_q_vi(w_hmc, mu_vi, sigma_vi)\n",
        "log_p_vals = log_posterior_hmc(w_hmc, X_train, y_train)\n",
        "kl_vi_hmc = np.mean(log_q_vals - log_p_vals)\n",
        "print(f\"Approximate KL(q_phi || p(w|D)) ≈ {kl_vi_hmc:.4f}\")\n",
        "\n",
        "# ================================\n",
        "# 4. Test-set log-loss\n",
        "# ================================\n",
        "def predictive_probs_vi(X, mu, sigma, n_samples=1000):\n",
        "    # Monte Carlo approximation of predictive probabilities\n",
        "    eps = np.random.randn(n_samples, D)\n",
        "    w_samples = mu + sigma * eps\n",
        "    logits = np.dot(X, w_samples.T)  # shape: (num_test, n_samples)\n",
        "    probs = expit(logits)\n",
        "    return np.mean(probs, axis=1)\n",
        "\n",
        "def predictive_probs_hmc(X, w_samples):\n",
        "    logits = np.dot(X, w_samples.T)\n",
        "    probs = expit(logits)\n",
        "    return np.mean(probs, axis=1)\n",
        "\n",
        "# Compute predictive probabilities\n",
        "y_pred_vi = predictive_probs_vi(X_test, mu_vi, sigma_vi, n_samples=1000)\n",
        "y_pred_hmc = predictive_probs_hmc(X_test, w_hmc)\n",
        "\n",
        "# Compute log-loss\n",
        "logloss_vi = log_loss(y_test, y_pred_vi)\n",
        "logloss_hmc = log_loss(y_test, y_pred_hmc)\n",
        "print(f\"Test-set Log-loss VI: {logloss_vi:.4f}\")\n",
        "print(f\"Test-set Log-loss HMC: {logloss_hmc:.4f}\")\n",
        "\n",
        "# ================================\n",
        "# 5. Results Table\n",
        "# ================================\n",
        "import pandas as pd\n",
        "results = pd.DataFrame({\n",
        "    \"Method\": [\"VI\", \"HMC\"],\n",
        "    \"KL(q||p) (approx)\": [kl_vi_hmc, np.nan],\n",
        "    \"Test Log-loss\": [logloss_vi, logloss_hmc]\n",
        "})\n",
        "print(results)\n",
        "\n",
        "# ================================\n",
        "# 6. Optional: Plot predictive probabilities comparison\n",
        "# ================================\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar([0,1], [logloss_vi, logloss_hmc], color=['skyblue','salmon'])\n",
        "plt.xticks([0,1], ['VI', 'HMC'])\n",
        "plt.ylabel(\"Test Log-loss\")\n",
        "plt.title(\"VI vs HMC Predictive Performance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw-agige31rk"
      },
      "source": [
        "**Question**\n",
        "\n",
        "**1.3.D Discussion (2 pt)**  \n",
        "   - Discuss where VI diverges from HMC. Comment on underestimation of uncertainty, mode-seeking bias, and the speed vs. accuracy trade-off.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZzppYK31rk"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "1. **Where VI diverges from HMC:**\n",
        "\n",
        "   * The **KL(qϕ‖p(w|D)) ≈ 2530** indicates that the variational posterior is quite far from the true posterior sampled by HMC.\n",
        "   * This large KL is typical in high-dimensional models because VI often fits a simpler distribution (here, a fully factorized Gaussian) compared to the potentially complex correlations captured by HMC.\n",
        "   * From the weight posteriors, VI captures the general location (mean) of most coefficients reasonably well, but the **variances are smaller** than HMC, indicating underestimation of uncertainty.\n",
        "\n",
        "2. **Underestimation of uncertainty:**\n",
        "\n",
        "   * VI produces narrower posterior distributions (**σ_vi ~ 0.065–0.075**) compared to the spread seen in HMC samples.\n",
        "   * This is a common phenomenon because VI minimizes KL(q‖p), which penalizes putting probability mass where the true posterior has none, leading to **mode-seeking behavior**.\n",
        "   * Consequently, VI is overconfident in parameter estimates, which may slightly degrade predictive performance in out-of-sample scenarios.\n",
        "\n",
        "3. **Mode-seeking bias:**\n",
        "\n",
        "   * VI tends to concentrate around one mode of the posterior, ignoring other possible modes if they exist.\n",
        "   * In contrast, HMC explores the full posterior landscape, capturing correlations and multi-modality, which is reflected in the slightly lower test log-loss (**0.3088 vs 0.3107** for VI).\n",
        "\n",
        "4. **Speed vs. accuracy trade-off:**\n",
        "\n",
        "   * VI is much faster to train and produces approximate posterior estimates without costly MCMC sampling.\n",
        "   * HMC provides highly accurate samples and captures full posterior uncertainty, but requires significantly more computation and memory, especially for multiple chains.\n",
        "   * For large datasets or real-time predictions, VI offers a practical compromise, trading some accuracy and uncertainty fidelity for speed.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "* VI is **fast, mode-seeking, and underestimates uncertainty**.\n",
        "* HMC is **accurate, captures full posterior correlations, but computationally expensive**.\n",
        "* The choice depends on whether **speed or posterior fidelity** is more critical for the task at hand.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWKwKL2QVO4Z"
      },
      "source": [
        "# 2. Gaussian processes and Bayesian Optimisation (15 marks)\n",
        "\n",
        "This part of the coursework uses Gaussian processes for Bayesian optimisation of the hyperparameters of a Machine Learning model. The dataset you will use in this assignment comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). The task is  to predict electrical energy output from a [combined cycle Power Plant](https://en.wikipedia.org/wiki/Combined_cycle_power_plant). The description of the dataset can be found [here](https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_sWEGZ31rk"
      },
      "source": [
        "## Bayesian optimisation of Elastic net hyperparameters\n",
        "\n",
        "You will use Thompson Sampling (TS) for hyperparemeter learning of an [elastic net model for regression](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net) over the electrical energy output dataset. Before moving on, please read the mathematical description of the Elastic net model [here](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net). The two hyperparameters to optimise will be the $\\alpha>0$ parameter and the $0\\le\\rho\\le1$ parameter. The $\\alpha$ and $\\rho$ hyperparameters are related to the level of $\\ell_1$ and $\\ell_2$ regularisaion done for the linear regression model. The exact relationship between these parameters is explained [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet).\n",
        "\n",
        "The function we want to minimise will be the Root Mean-Squared Error (RMSE) on a validation dataset,\n",
        "\n",
        "\\begin{align}\n",
        "    \\text{RMSE}(\\alpha, \\rho) = \\sqrt{\\text{MSE}(\\mathbf{y}_\\text{val}, f(\\mathbf{x}_{\\text{val}}, \\mathcal{D}_\\text{train}, \\alpha, \\rho))},\n",
        "\\end{align}\n",
        "\n",
        "where $\\text{MSE}(\\mathbf{y}_\\text{val}, f(\\mathbf{x}_{\\text{val}}, \\mathcal{D}_\\text{train}, \\alpha, \\rho))$ is the Mean-Squared error between the validation data and the prediction of the elastic net model, which in turn is a function of the training data, $\\mathcal{D}_\\text{train}$, and the hyperparameters $\\alpha$ and $\\rho$.\n",
        "\n",
        "**IMPORTANT. You can use scikit-learn for implementing the Elastic Net regressor. You can also use any library for the Gaussian process surrogate model. However, you CAN NOT use any package for the Bayesian optimisation loop. Failure to follow these instructions, will lead to a mark of zero for this section of the assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGJ3nMzc31rk"
      },
      "source": [
        "We will first load the dataset and split it into training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fPs6u1y731rk"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "doq = \"https://archive.ics.uci.edu/static/public/294/combined+cycle+power+plant.zip\"\n",
        "pat_sav = \"./combined+cycle+power+plant.zip\"\n",
        "urllib.request.urlretrieve(doq, pat_sav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lmmVrcoV31rk"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip = zipfile.ZipFile('./combined+cycle+power+plant.zip', 'r')\n",
        "for name in zip.namelist():\n",
        "    zip.extract(name, '.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "EEbecjzx31rl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "energy_output = pd.read_excel('./CCPP/Folds5x2_pp.xlsx','Sheet1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id4ziuIr31rl"
      },
      "source": [
        "The dataset has 9568 observations. We will use a subset of $N_m$ for this exercise. From those, we will select $80\\%$ as the training data, $10\\%$ as the validation data, and $10\\%$ as the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Kzmny_Sf31rl"
      },
      "outputs": [],
      "source": [
        "N_m = 9000\n",
        "ndata, ncols = np.shape(energy_output)\n",
        "np.random.seed(22222)                 # Make sure you use the last five digits of your student UCard as your seed\n",
        "index = np.random.permutation(ndata)  # We permute the indexes\n",
        "data_tot_red = energy_output.iloc[index[0:N_m], :].copy() # Select N_m points\n",
        "Ne = np.int64(np.round(0.8*N_m))    # We compute N, the number of training instances\n",
        "Neval = np.int64(np.round(0.1*N_m)) # We compute Nval, the number of validation instances\n",
        "Netest = N_m - Ne - Neval              # We compute Ntest, the number of test instances\n",
        "index = np.random.permutation(N_m)  # We permute the indexes\n",
        "data_training = data_tot_red.iloc[index[0:Ne], :].copy() # Select the training data\n",
        "data_val = data_tot_red.iloc[index[Ne:Ne+Neval], :].copy() # Select the validation data\n",
        "data_test = data_tot_red.iloc[index[Ne+Neval:N_m], :].copy() # Select the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wU-HNekC31rl"
      },
      "outputs": [],
      "source": [
        "Xe_train = np.concatenate((np.ones((Ne,1)), (data_training.iloc[:, 0:4]).values), axis=1)\n",
        "ye_train = np.reshape((data_training.iloc[:, 4]).values, (Ne,1))\n",
        "Xe_val = np.concatenate((np.ones((Neval,1)), (data_val.iloc[:, 0:4]).values), axis=1)\n",
        "ye_val = np.reshape((data_val.iloc[:, 4]).values, (Neval,1))\n",
        "Xe_test = np.concatenate((np.ones((Netest,1)), (data_test.iloc[:, 0:4]).values), axis=1)\n",
        "ye_test = np.reshape((data_test.iloc[:, 4]).values, (Netest,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAox0v8t31rl"
      },
      "source": [
        "### 2.1 Initial space filling-design (5 marks)\n",
        "\n",
        "We start by collecting a few initial points from the function we want to optimise. Here, you will assume the first initial design has $n_0 = 5$ points. The ouput of this part of your code should be the data observations ``X0train`` and ``y0train``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXUjCNhC31rl"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "id": "DZrJoz_Y31rl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad75339-733d-41c7-ea66-cf916bcdb728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial design hyperparameters (alpha, rho):\n",
            " [[0.68506976 0.99083973]\n",
            " [3.12568677 0.12058048]\n",
            " [1.95606645 0.73696298]\n",
            " [1.28961767 0.56141724]\n",
            " [3.22944268 0.51684148]]\n",
            "Initial design RMSE on validation set:\n",
            " [[4.45009463]\n",
            " [4.61082654]\n",
            " [4.48351524]\n",
            " [4.46291942]\n",
            " [4.57008355]]\n"
          ]
        }
      ],
      "source": [
        "# Write the code to get n_0=5 points for the initial design\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Number of initial points\n",
        "n0 = 5\n",
        "\n",
        "# Define ranges for alpha (>0) and rho (0 <= rho <= 1)\n",
        "alpha_range = [1e-4, 10]  # Example range for alpha\n",
        "rho_range = [0.0, 1.0]    # Example range for l1_ratio\n",
        "\n",
        "# Randomly sample n0 points from the hyperparameter space\n",
        "np.random.seed(22222)  # Seed for reproducibility\n",
        "alpha_samples = np.random.uniform(alpha_range[0], alpha_range[1], n0)\n",
        "rho_samples = np.random.uniform(rho_range[0], rho_range[1], n0)\n",
        "\n",
        "# Store X0train and y0train\n",
        "X0train = np.column_stack((alpha_samples, rho_samples))\n",
        "y0train = []\n",
        "\n",
        "# Evaluate RMSE for each hyperparameter setting\n",
        "for i in range(n0):\n",
        "    alpha_i = X0train[i, 0]\n",
        "    rho_i = X0train[i, 1]\n",
        "\n",
        "    # Train Elastic Net on training data\n",
        "    model = ElasticNet(alpha=alpha_i, l1_ratio=rho_i, random_state=22222, max_iter=10000)\n",
        "    model.fit(Xe_train, ye_train.ravel())\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_val_pred = model.predict(Xe_val)\n",
        "\n",
        "    # Compute RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(ye_val, y_val_pred))\n",
        "    y0train.append(rmse)\n",
        "\n",
        "y0train = np.array(y0train).reshape(-1, 1)\n",
        "\n",
        "print(\"Initial design hyperparameters (alpha, rho):\\n\", X0train)\n",
        "print(\"Initial design RMSE on validation set:\\n\", y0train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSSigF_p31rl"
      },
      "source": [
        "### 2.2 Implement the sequentail-decision loop to find the optimal set of hyperparameters (5 marks)\n",
        "\n",
        "Assume your optimisation budget is equal to $N = 20$ function evaluations and implement your Bayesian optimiser. For each iteration in the optimisation, you need to:\n",
        "\n",
        "1. Compute the posterior distribution of your Gaussian process using all available training data.\n",
        "2. Use Thompson sampling to find the optimal value to explore next.\n",
        "3. Observe your new output at the suggested optimal point.\n",
        "\n",
        "At the end of the optimisation loop, you should return a value of $\\alpha_*$ and $\\rho_*$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrHyhkUU31rm"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": true,
        "id": "wZH5-4J931rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0b93ff-0cbc-45ef-de87-6ccc8e5fb37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6: alpha=8.4286, rho=0.0907, RMSE=5.0710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7: alpha=0.0351, rho=0.1254, RMSE=4.4434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8: alpha=0.0716, rho=0.5314, RMSE=4.4436\n",
            "Iteration 9: alpha=0.0525, rho=0.6969, RMSE=4.4438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10: alpha=0.1984, rho=0.9464, RMSE=4.4448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11: alpha=0.1337, rho=0.3937, RMSE=4.4431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12: alpha=0.0172, rho=0.3377, RMSE=4.4438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 13: alpha=0.2094, rho=0.2207, RMSE=4.4425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 14: alpha=0.2827, rho=0.0125, RMSE=4.4422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15: alpha=0.4802, rho=0.3037, RMSE=4.4445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16: alpha=0.2777, rho=0.1175, RMSE=4.4424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 17: alpha=0.1623, rho=0.2648, RMSE=4.4427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 18: alpha=0.4164, rho=0.0067, RMSE=4.4437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 19: alpha=0.2294, rho=0.1263, RMSE=4.4423\n",
            "Iteration 20: alpha=0.3318, rho=0.4598, RMSE=4.4435\n",
            "\n",
            "Optimal hyperparameters found:\n",
            "alpha* = 0.282673, rho* = 0.012523, RMSE* = 4.4422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(22222)\n",
        "\n",
        "# Optimization budget\n",
        "N = 20  # Total number of function evaluations\n",
        "\n",
        "# Bounds for hyperparameters\n",
        "alpha_bounds = [1e-4, 10]\n",
        "rho_bounds = [0.0, 1.0]\n",
        "\n",
        "# Start with initial design\n",
        "X_train = X0train.copy()  # Shape: (n0, 2)\n",
        "y_train = y0train.copy()  # Shape: (n0, 1)\n",
        "\n",
        "# Gaussian Process kernel\n",
        "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(2), nu=2.5) + WhiteKernel(noise_level=1e-6)\n",
        "\n",
        "# Sequential optimization loop\n",
        "for t in range(len(X_train), N):\n",
        "    # Fit GP on current data\n",
        "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, normalize_y=True, random_state=22222)\n",
        "    gp.fit(X_train, y_train.ravel())\n",
        "\n",
        "    # Thompson Sampling: sample function from GP posterior\n",
        "    n_candidates = 10000  # Number of random candidates to explore\n",
        "    alpha_candidates = np.random.uniform(alpha_bounds[0], alpha_bounds[1], n_candidates)\n",
        "    rho_candidates = np.random.uniform(rho_bounds[0], rho_bounds[1], n_candidates)\n",
        "    X_candidates = np.column_stack((alpha_candidates, rho_candidates))\n",
        "\n",
        "    # Predict mean and covariance of GP at candidate points\n",
        "    y_mean, y_std = gp.predict(X_candidates, return_std=True)\n",
        "\n",
        "    # Sample from the GP posterior at candidate points\n",
        "    y_sample = np.random.normal(y_mean, y_std)\n",
        "\n",
        "    # Select next point to evaluate: minimizer of sampled function\n",
        "    next_idx = np.argmin(y_sample)\n",
        "    next_alpha, next_rho = X_candidates[next_idx]\n",
        "\n",
        "    # Evaluate Elastic Net at the suggested hyperparameters\n",
        "    model = ElasticNet(alpha=next_alpha, l1_ratio=next_rho, random_state=22222, max_iter=10000)\n",
        "    model.fit(Xe_train, ye_train.ravel())\n",
        "    y_val_pred = model.predict(Xe_val)\n",
        "    rmse_next = np.sqrt(mean_squared_error(ye_val, y_val_pred))\n",
        "\n",
        "    # Append new observation to training data\n",
        "    X_train = np.vstack([X_train, [next_alpha, next_rho]])\n",
        "    y_train = np.vstack([y_train, [rmse_next]])\n",
        "\n",
        "    print(f\"Iteration {t+1}: alpha={next_alpha:.4f}, rho={next_rho:.4f}, RMSE={rmse_next:.4f}\")\n",
        "\n",
        "# After the loop, return the best hyperparameters found\n",
        "best_idx = np.argmin(y_train)\n",
        "alpha_star, rho_star = X_train[best_idx]\n",
        "best_rmse = y_train[best_idx, 0]\n",
        "\n",
        "print(\"\\nOptimal hyperparameters found:\")\n",
        "print(f\"alpha* = {alpha_star:.6f}, rho* = {rho_star:.6f}, RMSE* = {best_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44dI_T0m31rm"
      },
      "source": [
        "### 2.3 Compute RMSE in the test set and compare the performance of the Bayes Opt approach against an alterntive method for hyperparameter selection (5 marks)\n",
        "\n",
        "Compute the performance of the Elastic Net model over the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41F9SYDf31rm"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_yNXw3Go31rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894d9c64-17b5-43eb-9a97-47a627d36cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE with Bayesian Optimisation hyperparameters: 4.7760\n",
            "Test RMSE with default hyperparameters: 4.7831\n",
            "Improvement in RMSE by Bayesian Optimisation: 0.0070\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import numpy as np\n",
        "\n",
        "# Optimal hyperparameters from Bayesian Optimization\n",
        "alpha_opt = alpha_star\n",
        "rho_opt = rho_star\n",
        "\n",
        "# 1. Evaluate Elastic Net on test set using optimal hyperparameters\n",
        "model_opt = ElasticNet(alpha=alpha_opt, l1_ratio=rho_opt, random_state=22222, max_iter=10000)\n",
        "model_opt.fit(Xe_train, ye_train.ravel())\n",
        "y_test_pred_opt = model_opt.predict(Xe_test)\n",
        "rmse_test_opt = np.sqrt(mean_squared_error(ye_test, y_test_pred_opt))\n",
        "\n",
        "print(f\"Test RMSE with Bayesian Optimisation hyperparameters: {rmse_test_opt:.4f}\")\n",
        "\n",
        "# 2. Evaluate Elastic Net on test set using default hyperparameters as alternative\n",
        "model_default = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=22222, max_iter=10000)\n",
        "model_default.fit(Xe_train, ye_train.ravel())\n",
        "y_test_pred_default = model_default.predict(Xe_test)\n",
        "rmse_test_default = np.sqrt(mean_squared_error(ye_test, y_test_pred_default))\n",
        "\n",
        "print(f\"Test RMSE with default hyperparameters: {rmse_test_default:.4f}\")\n",
        "\n",
        "# 3. Compare performances\n",
        "improvement = rmse_test_default - rmse_test_opt\n",
        "print(f\"Improvement in RMSE by Bayesian Optimisation: {improvement:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt5X3k2331rm"
      },
      "source": [
        "Use an alternative approach for hyperparameter learning of $\\alpha$ and $\\rho$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc_bdgg531rm"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lTbjpCt531rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f9e7b3-7082-406d-e192-fddfb15f2eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal hyperparameters from Grid Search: alpha=0.002783, rho=0.555556\n",
            "Test RMSE with Grid Search hyperparameters: 4.7756\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Define the grid of hyperparameters to search\n",
        "param_grid = {\n",
        "    'alpha': np.logspace(-3, 1, 10),   # From 0.001 to 10\n",
        "    'l1_ratio': np.linspace(0, 1, 10)  # From 0 (Ridge) to 1 (Lasso)\n",
        "}\n",
        "\n",
        "# Create ElasticNet model\n",
        "elastic_net = ElasticNet(random_state=22222, max_iter=10000)\n",
        "\n",
        "# Use GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=elastic_net,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error',  # maximize negative MSE\n",
        "                           cv=5,\n",
        "                           n_jobs=-1)\n",
        "\n",
        "# Fit to training data\n",
        "grid_search.fit(Xe_train, ye_train.ravel())\n",
        "\n",
        "# Get the best hyperparameters\n",
        "alpha_grid = grid_search.best_params_['alpha']\n",
        "rho_grid = grid_search.best_params_['l1_ratio']\n",
        "print(f\"Optimal hyperparameters from Grid Search: alpha={alpha_grid:.6f}, rho={rho_grid:.6f}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "model_grid = ElasticNet(alpha=alpha_grid, l1_ratio=rho_grid, random_state=22222, max_iter=10000)\n",
        "model_grid.fit(Xe_train, ye_train.ravel())\n",
        "y_test_pred_grid = model_grid.predict(Xe_test)\n",
        "rmse_test_grid = np.sqrt(mean_squared_error(ye_test, y_test_pred_grid))\n",
        "\n",
        "print(f\"Test RMSE with Grid Search hyperparameters: {rmse_test_grid:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt201uEZ31rm"
      },
      "source": [
        "Write and discuss two interesting findings of this experiment.\n",
        "\n",
        "- *Interesting finding 1*. Write a sentence here of no more than **30 words**.\n",
        "- *Interesting finding 2*. Write a sentence here of no more than **30 words**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfIMKFlp31rn"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "Interesting finding 1: Bayesian optimisation efficiently found hyperparameters near the global optimum, achieving slightly better RMSE than default settings with fewer evaluations.\n",
        "\n",
        "Interesting finding 2: Grid search produced comparable RMSE to Bayesian optimisation, showing that exhaustive search can be effective but is computationally more expensive."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}